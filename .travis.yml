language: java
jdk: oraclejdk8

script:
  # set up Hadoop
  - HADOOP_VERSION=2.7.2
  - HADOOP_ARCHIVE=$HOME/hadoop-$HADOOP_VERSION.tar.gz
  - wget --no-verbose --output-document $HADOOP_ARCHIVE http://archive.apache.org/dist/hadoop/core/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz
  - tar --extract --file $HADOOP_ARCHIVE --directory $HOME
  - HADOOP_HOME=$HOME/hadoop-$HADOOP_VERSION
  - rm $HADOOP_ARCHIVE
  - cp src/test/resources/hadoop/core-site.xml $HADOOP_HOME/etc/hadoop
  - cp src/test/resources/hadoop/hdfs-site.xml $HADOOP_HOME/etc/hadoop

  # accept own key to start services locally via ssh
  - mkdir -p $HOME/.ssh
  - ssh-keygen -t dsa -P '' -f $HOME/.ssh/id_dsa
  - cat $HOME/.ssh/id_dsa.pub >> $HOME/.ssh/authorized_keys
  - chmod 600 $HOME/.ssh/authorized_keys
  - ssh-keyscan -t dsa localhost >> $HOME/.ssh/known_hosts
  - ssh-keyscan -t dsa 0.0.0.0 >> $HOME/.ssh/known_hosts

  # build and install the adapter
  - mvn package
  - cp target/hdfs-statistics-adapter-1.0-SNAPSHOT.jar $HADOOP_HOME/share/hadoop/common

  # start Hadoop
  - $HADOOP_HOME/bin/hdfs namenode -format
  - $HADOOP_HOME/sbin/start-dfs.sh

  # run sample grep job
  - $HADOOP_HOME/bin/hdfs dfs -mkdir hdfs:///user
  - $HADOOP_HOME/bin/hdfs dfs -mkdir hdfs:///user/$USER
  - $HADOOP_HOME/bin/hdfs dfs -put $HADOOP_HOME/etc/hadoop hdfs:///user/$USER/input
  - $HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-$HADOOP_VERSION.jar grep input output 'dfs[a-z.]+'
  - $HADOOP_HOME/bin/hdfs dfs -cat hdfs:///user/$USER/output/*

  # stop Hadoop
  - $HADOOP_HOME/sbin/stop-dfs.sh