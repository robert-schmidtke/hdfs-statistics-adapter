{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import Legend, LinearAxis, Range1d, Title\n",
    "from bokeh.palettes import Spectral8\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "# increase default cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "\n",
    "# for bokeh in notebooks\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read all CSV files into one DataFrame\n",
    "benchmarks = [\n",
    "#    'terasort-hadoop-1T-16' # ,\n",
    "#    'terasort-flink-1T-16' #,\n",
    "#    'terasort-flink-640G-16-2'\n",
    "#    'peakpicking-flink-205G-16'\n",
    "    'terasort-hadoop-1T-16'\n",
    "]\n",
    "\n",
    "all_data = dict()\n",
    "for benchmark in benchmarks:\n",
    "    csv_directory = \"../resources/data/{}\".format(benchmark)\n",
    "    print(\"Reading data from {}\".format(csv_directory))\n",
    "    all_data[benchmark] = pd.DataFrame()\n",
    "    \n",
    "    csvs = os.listdir(csv_directory)\n",
    "    i = 0\n",
    "    for csv in csvs:\n",
    "        if csv.endswith('csv') and os.path.isfile(csv_directory + '/' + csv):\n",
    "            all_data[benchmark] = all_data[benchmark].append(pd.read_csv(csv_directory + '/' + csv)).fillna(0)\n",
    "print(\"Done reading all input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aggregate over all pids of desired hosts the metrics per time, key, source and category\n",
    "hostnames = [\n",
    "    'cumu01-00',\n",
    "    'cumu01-01',\n",
    "    'cumu01-02',\n",
    "    'cumu01-03',\n",
    "    'cumu01-04',\n",
    "    'cumu01-05',\n",
    "    'cumu01-06',\n",
    "    'cumu01-07',\n",
    "    'cumu01-08',\n",
    "    'cumu01-09',\n",
    "    'cumu01-10',\n",
    "    'cumu01-11',\n",
    "    'cumu01-12',\n",
    "    'cumu01-13',\n",
    "    'cumu01-14',\n",
    "    'cumu01-15',\n",
    "    'cumu02-00',\n",
    "    'cumu02-01',\n",
    "    'cumu02-02',\n",
    "    'cumu02-03',\n",
    "    'cumu02-04',\n",
    "    'cumu02-05',\n",
    "    'cumu02-06',\n",
    "    'cumu02-07',\n",
    "    'cumu02-08',\n",
    "    'cumu02-09',\n",
    "    'cumu02-10',\n",
    "    'cumu02-11',\n",
    "    'cumu02-12',\n",
    "    'cumu02-13',\n",
    "    'cumu02-14',\n",
    "    'cumu02-15'\n",
    "]\n",
    "\n",
    "grouped_data = dict()\n",
    "for benchmark in benchmarks:\n",
    "    grouped_data[benchmark] = all_data[benchmark][all_data[benchmark]['hostname'].isin(hostnames)].drop(['hostname', 'pid'], axis=1).groupby(['timeBin', 'key', 'source', 'category'], as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# regroup to obtain a DataFrame per key/source/category tuple\n",
    "regrouped_data = dict()\n",
    "for benchmark in benchmarks:\n",
    "    regrouped_data[benchmark] = grouped_data[benchmark].groupby(['key', 'source', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select framework to examine\n",
    "current_data = regrouped_data['terasort-hadoop-1T-16']\n",
    "\n",
    "# one figure for each key, same for legends\n",
    "\n",
    "# depict total data per time bin, cumulative data\n",
    "data_figures = dict()\n",
    "data_legends = dict()\n",
    "\n",
    "# depict average data per time bin\n",
    "avg_data_figures = dict()\n",
    "avg_data_legends = dict()\n",
    "\n",
    "# depict number of operations per time bin, cumulative number of operations\n",
    "count_figures = dict()\n",
    "count_legends = dict()\n",
    "\n",
    "# we will need at most eight colors per plot\n",
    "colors = {\n",
    "    ('jvm', 'read') : Spectral8[0],\n",
    "    ('jvm', 'write') : Spectral8[1],\n",
    "    ('jvm', 'other') : Spectral8[2],\n",
    "    ('jvm', 'zip') : Spectral8[3],\n",
    "    ('sfs', 'read') : Spectral8[4],\n",
    "    ('sfs', 'write') : Spectral8[5],\n",
    "    ('sfs', 'other') : Spectral8[6],\n",
    "    ('sfs', 'zip') : Spectral8[7]\n",
    "}\n",
    "\n",
    "# remember max left and right y values for each plot\n",
    "max_data = dict()\n",
    "max_cum_data = dict()\n",
    "max_avg_data = dict()\n",
    "max_count = dict()\n",
    "max_cum_count = dict()\n",
    "\n",
    "# remember x range so all plots have the same extent\n",
    "min_x = 9223372036854775807\n",
    "max_x = 0\n",
    "\n",
    "# loop over all unique groups\n",
    "for group in current_data.groups:\n",
    "    # group[0] contains the key, e.g. hadoop, spark, yarn, ...\n",
    "    # group[1] contains jvm or sfs\n",
    "    # group[2] contains read, write or other\n",
    "    \n",
    "    # skip group if we do not want to plot it\n",
    "    if not (group[1], group[2]) in colors:\n",
    "        continue\n",
    "    \n",
    "    # get this group's data\n",
    "    group_data = current_data.get_group(group)\n",
    "    \n",
    "    # start with data related plots, so skip other\n",
    "    if group[2] != 'other':\n",
    "        data_figure = data_figures.get(\n",
    "            group[0],\n",
    "            figure(\n",
    "                plot_width=1300,\n",
    "                title=\"{}: data\".format(group[0]),\n",
    "                title_location='above',\n",
    "                toolbar_location='left',\n",
    "                toolbar_sticky=False,\n",
    "                x_axis_type='datetime'\n",
    "            )\n",
    "        )\n",
    "        data_figures[group[0]] = data_figure\n",
    "\n",
    "        # remember maximum data for plotting later\n",
    "        max_data[group[0]] = max([max_data.get(group[0], 0), group_data['data'].max()])\n",
    "\n",
    "        # remember x range\n",
    "        min_x = min(min_x, group_data['timeBin'].min())\n",
    "        max_x = max(max_x, group_data['timeBin'].max())\n",
    "\n",
    "        # get cumulative sums on data for this group, and remember the maximum\n",
    "        cum_data = group_data['data'].cumsum()\n",
    "        max_cum_data[group[0]] = max([max_cum_data.get(group[0], 0), cum_data.max()])\n",
    "\n",
    "        # timeBin is in milliseconds, data is in bytes\n",
    "        # convert to seconds and megabytes\n",
    "        # and look up color for source/category combination\n",
    "        data_line = data_figure.line(\n",
    "            x=[dt.datetime.fromtimestamp(s / 1000) for s in group_data['timeBin']],\n",
    "            y=[d / 1048576 for d in group_data['data']],\n",
    "            color = colors[(group[1], group[2])]\n",
    "        )\n",
    "\n",
    "        # plot cumulative data for the same source/category, but on the other y-range\n",
    "        cum_data_line = data_figure.line(\n",
    "            x=[dt.datetime.fromtimestamp(s / 1000) for s in group_data['timeBin']],\n",
    "            y=[d / 1073741824 for d in cum_data],\n",
    "            color = colors[(group[1], group[2])],\n",
    "            y_range_name='cum_data_range'\n",
    "        )\n",
    "\n",
    "        # save the legend for this line, simple concatenation of key, source and category\n",
    "        data_legend = data_legends.get(group[0], [])\n",
    "        data_legend.append((\"{}: {}\".format(group[1], group[2]), [data_line, cum_data_line]))\n",
    "        data_legends[group[0]] = data_legend\n",
    "    \n",
    "        # now for average data operation size\n",
    "        avg_data_figure = avg_data_figures.get(\n",
    "            group[0],\n",
    "            figure(\n",
    "                plot_width=1300,\n",
    "                title=\"{}: avg. data\".format(group[0]),\n",
    "                title_location='above',\n",
    "                toolbar_location='left',\n",
    "                toolbar_sticky=False,\n",
    "                x_axis_type='datetime'\n",
    "            )\n",
    "        )\n",
    "        avg_data_figures[group[0]] = avg_data_figure\n",
    "        \n",
    "        avg_data = group_data['data'] / group_data['count']\n",
    "        \n",
    "        # remember max average data\n",
    "        max_avg_data[group[0]] = max(max_avg_data.get(group[0], 0), avg_data.max())\n",
    "        \n",
    "        avg_data_line = avg_data_figure.line(\n",
    "            x=[dt.datetime.fromtimestamp(s / 1000) for s in group_data['timeBin']],\n",
    "            y=[d / 1048576 for d in avg_data],\n",
    "            color = colors[(group[1], group[2])]\n",
    "        )\n",
    "        \n",
    "        avg_data_legend = avg_data_legends.get(group[0], [])\n",
    "        avg_data_legend.append((\"{}: {}\".format(group[1], group[2]), [avg_data_line]))\n",
    "        avg_data_legends[group[0]] = avg_data_legend\n",
    "        \n",
    "    # now for the number of operations\n",
    "    count_figure = count_figures.get(\n",
    "        group[0],\n",
    "        figure(\n",
    "            plot_width=1300,\n",
    "            title=\"{}: count\".format(group[0]),\n",
    "            title_location='above',\n",
    "            toolbar_location='left',\n",
    "            toolbar_sticky=False,\n",
    "            x_axis_type='datetime'\n",
    "        )\n",
    "    )\n",
    "    count_figures[group[0]] = count_figure\n",
    "    \n",
    "    # remember max count for scaling\n",
    "    max_count[group[0]] = max(max_count.get(group[0], 0), group_data['count'].max())\n",
    "    \n",
    "    # get cumulative count of operations for this group\n",
    "    cum_count = group_data['count'].cumsum()\n",
    "    max_cum_count[group[0]] = max(max_cum_count.get(group[0], 0), cum_count.max())\n",
    "    \n",
    "    count_line = count_figure.line(\n",
    "        x=[dt.datetime.fromtimestamp(s / 1000) for s in group_data['timeBin']],\n",
    "        y=group_data['count'],\n",
    "        color = colors[(group[1], group[2])]\n",
    "    )\n",
    "    \n",
    "    cum_count_line = count_figure.line(\n",
    "        x=[dt.datetime.fromtimestamp(s / 1000) for s in group_data['timeBin']],\n",
    "        y=cum_count,\n",
    "        color = colors[(group[1], group[2])],\n",
    "        y_range_name='cum_count_range'\n",
    "    )\n",
    "    \n",
    "    count_legend = count_legends.get(group[0], [])\n",
    "    count_legend.append((\"{}: {}\".format(group[1], group[2]), [count_line, cum_count_line]))\n",
    "    count_legends[group[0]] = count_legend\n",
    "\n",
    "# now generate all figures and their legends\n",
    "plots = []\n",
    "for key, f in data_figures.items():\n",
    "    x_range_margin = (max_x - min_x) * 0.05\n",
    "    f.x_range = Range1d(start=min_x - x_range_margin, end=max_x + x_range_margin)\n",
    "    \n",
    "    y_range_margin = 0.05 * (max_data[key] / 1048576)\n",
    "    f.y_range = Range1d(start=-y_range_margin, end=y_range_margin + max_data[key] / 1048576)\n",
    "    \n",
    "    f.add_layout(LinearAxis(y_range_name='cum_data_range'), 'right')\n",
    "    extra_y_range_margin = 0.05 * max_cum_data[key] / 1073741824\n",
    "    f.extra_y_ranges = { 'cum_data_range' : Range1d(start=-extra_y_range_margin, end=extra_y_range_margin + max_cum_data[key] / 1073741824) }\n",
    "    \n",
    "    f.title.align = 'center'\n",
    "    f.xaxis[0].axis_label = \"Time\"\n",
    "    f.yaxis[0].axis_label = \"Data (MiB)\"\n",
    "    f.yaxis[1].axis_label = \"Cum. Data (GiB)\"\n",
    "    \n",
    "    f.add_layout(Legend(items=data_legends.get(key), location=(0, -30)), 'right')\n",
    "    \n",
    "    plots.append(f)\n",
    "\n",
    "for key, f in avg_data_figures.items():\n",
    "    x_range_margin = (max_x - min_x) * 0.05\n",
    "    f.x_range = Range1d(start=min_x - x_range_margin, end=max_x + x_range_margin)\n",
    "    \n",
    "    y_range_margin = 0.05 * (max_avg_data[key] / 1048576)\n",
    "    f.y_range = Range1d(start=-y_range_margin, end=y_range_margin + max_avg_data[key] / 1048576)\n",
    "    \n",
    "    f.title.align = 'center'\n",
    "    f.xaxis[0].axis_label = \"Time\"\n",
    "    f.yaxis[0].axis_label = \"Data (MiB)\"\n",
    "    \n",
    "    f.add_layout(Legend(items=avg_data_legends.get(key), location=(0, -30)), 'right')\n",
    "    \n",
    "    plots.append(f)\n",
    "\n",
    "for key, f in count_figures.items():\n",
    "    x_range_margin = (max_x - min_x) * 0.05\n",
    "    f.x_range = Range1d(start=min_x - x_range_margin, end=max_x + x_range_margin)\n",
    "    \n",
    "    y_range_margin = 0.05 * (max_count[key])\n",
    "    f.y_range = Range1d(start=-y_range_margin, end=y_range_margin + max_count[key])\n",
    "    \n",
    "    f.add_layout(LinearAxis(y_range_name='cum_count_range'), 'right')\n",
    "    extra_y_range_margin = 0.05 * max_cum_count[key]\n",
    "    f.extra_y_ranges = { 'cum_count_range' : Range1d(start=-extra_y_range_margin, end=extra_y_range_margin + max_cum_count[key]) }\n",
    "    \n",
    "    f.title.align = 'center'\n",
    "    f.xaxis[0].axis_label = \"Time\"\n",
    "    f.yaxis[0].axis_label = \"Count\"\n",
    "    f.yaxis[1].axis_label = \"Cum. Count\"\n",
    "    \n",
    "    f.add_layout(Legend(items=count_legends.get(key), location=(0, -30)), 'right')\n",
    "    \n",
    "    plots.append(f)\n",
    "\n",
    "# show all plots in a column for proper alignment of x-axis\n",
    "show(column(plots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
