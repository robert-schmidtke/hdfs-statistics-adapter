# Takes the path to an extracted benchmark run which contains CSV files for each source/category.
# Parses them and plots them into a cumulative stacked chart.
# Parses the slurm.out file and prints I/O for each phase.

import argparse
import datetime as dt
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import sys

# get the benchmark's directory containing all files
parser = argparse.ArgumentParser()
parser.add_argument("bd", help="path to the benchmark directory")
args = parser.parse_args()

# read all CSV files into one DataFrame
raw_data = pd.DataFrame()
files = os.listdir(args.bd)
slurm_file = None
for f in files:
    if ('sfs' in f or 'jvm' in f) and f.endswith('csv'):
        print("Reading {} ...".format(f))
        raw_data = raw_data.append(pd.read_csv(args.bd + '/' + f)).fillna(0)
    elif 'slurm' in f and f.endswith('out'):
        print("Reading {} ...".format(f))
        slurm_file = args.bd + '/' + f

if slurm_file is None:
    raise Exception('No slurm.out file found')

# parse start and end times from the log file, as well as interesting I/O
slurm = {
    'teragen' : {
        'time' : {
            'start' : None,
            'end'   : None
        },
        'io' : {
            'hdfs' : {
                'read'  : None,
                'write' : None
            },
            'file' : {
                'read'  : None,
                'write' : None
            },
            'shuffle' : {
                'read'  : 0,
                'write' : 0
            },
            'spill' : None
        }
    },
    'terasort' : {
        'time' : {
            'start' : None,
            'end'   : None
        },
        'io' : {
            'hdfs' : {
                'read'  : None,
                'write' : None
            },
            'file' : {
                'read'  : None,
                'write' : None
            },
            'shuffle' : {
                'read'  : None,
                'write' : None
            },
            'spill' : None
        }
    }
}

with open(slurm_file) as f:
    for line in f:
        # removes leading spaces and trailing newline
        line = line.strip()

        # messages generated by us, extract start and end time for TeraGen/TeraSort from them
        if line.endswith(": Generating TeraSort data on HDFS"):
            date = line[:-len(": Generating TeraSort data on HDFS")].replace("  ", " ")
            slurm['teragen']['time']['start'] = dt.datetime.strptime(date, "%a %b %d %H:%M:%S %Z %Y").timestamp() 
        elif line.endswith(": Generating TeraSort data on HDFS done"):
            date = date = line[:-len(": Generating TeraSort data on HDFS done")].replace("  ", " ")
            slurm['teragen']['time']['end'] = dt.datetime.strptime(date, "%a %b %d %H:%M:%S %Z %Y").timestamp()
        elif line.endswith(": Running TeraSort"):
            date = line[:-len(": Running TeraSort")].replace("  ", " ")
            slurm['terasort']['time']['start'] = dt.datetime.strptime(date, "%a %b %d %H:%M:%S %Z %Y").timestamp()
        elif line.endswith(": Running TeraSort done: 0"):
            date = line[:-len(": Running TeraSort done: 0")].replace("  ", " ")
            slurm['terasort']['time']['end'] = dt.datetime.strptime(date, "%a %b %d %H:%M:%S %Z %Y").timestamp()
        # MapReduce framework counters
        elif line.startswith("HDFS: Number of bytes read="):
            # first encounter is TeraGen
            if slurm['teragen']['io']['hdfs']['read'] == None:
                slurm['teragen']['io']['hdfs']['read'] = int(round(int(line.split("=")[1]) / 1073741824.0))
            # second encounter is TeraSort
            elif slurm['terasort']['io']['hdfs']['read'] == None:
                slurm['terasort']['io']['hdfs']['read'] = int(round(int(line.split("=")[1]) / 1073741824.0))
        elif line.startswith("HDFS: Number of bytes written="):
            if slurm['teragen']['io']['hdfs']['write'] == None:
                slurm['teragen']['io']['hdfs']['write'] = int(round(int(line.split("=")[1]) / 1073741824.0))
            elif slurm['terasort']['io']['hdfs']['write'] == None:
                slurm['terasort']['io']['hdfs']['write'] = int(round(int(line.split("=")[1]) / 1073741824.0))
        elif line.startswith("FILE: Number of bytes read="):
            if slurm['teragen']['io']['file']['read'] == None:
                slurm['teragen']['io']['file']['read'] = int(round(int(line.split("=")[1]) / 1073741824.0))
            elif slurm['terasort']['io']['file']['read'] == None:
                slurm['terasort']['io']['file']['read'] = int(round(int(line.split("=")[1]) / 1073741824.0))
        elif line.startswith("FILE: Number of bytes written="):
            if slurm['teragen']['io']['file']['write'] == None:
                slurm['teragen']['io']['file']['write'] = int(round(int(line.split("=")[1]) / 1073741824.0))
            elif slurm['terasort']['io']['file']['write'] == None:
                slurm['terasort']['io']['file']['write'] = int(round(int(line.split("=")[1]) / 1073741824.0))
        elif line.startswith("Reduce shuffle bytes="):
            slurm['terasort']['io']['shuffle']['read'] = int(round(int(line.split("=")[1]) / 1073741824.0))
        elif line.startswith("Map output materialized bytes="):
            slurm['terasort']['io']['shuffle']['write'] = int(round(int(line.split("=")[1]) / 1073741824.0))
        elif line.startswith("Spilled Records="):
            if slurm['teragen']['io']['spill'] == None:
                slurm['teragen']['io']['spill'] = int(round(int(line.split("=")[1]) * 100 / 1073741824.0))
            elif slurm['terasort']['io']['spill'] == None:
                slurm['terasort']['io']['spill'] = int(round(int(line.split("=")[1]) * 100 / 1073741824.0))

# use beginning of TeraGen as zero
slurm['terasort']['time']['end']   -= slurm['teragen']['time']['start']
slurm['terasort']['time']['start'] -= slurm['teragen']['time']['start']
slurm['teragen']['time']['end']    -= slurm['teragen']['time']['start']
slurm['teragen']['time']['start']   = 0

# print stats
print("")

print("TeraGen")
print("=======")
print("Duration:      {} minutes".format(int(round((slurm['teragen']['time']['end'] - slurm['teragen']['time']['start']) / 60.0))))
print("HDFS Read:     {} GiB".format(slurm['teragen']['io']['hdfs']['read']))
print("HDFS Write:    {} GiB".format(slurm['teragen']['io']['hdfs']['write']))
print("FILE Read:     {} GiB".format(slurm['teragen']['io']['file']['read']))
print("FILE Write:    {} GiB".format(slurm['teragen']['io']['file']['write']))
print("Shuffle Read:  {} GiB".format(slurm['teragen']['io']['shuffle']['read']))
print("Shuffle Write: {} GiB".format(slurm['teragen']['io']['shuffle']['write']))
print("Spill:         {} GiB".format(slurm['teragen']['io']['spill']))

print("")

print("TeraSort")
print("=======")
print("Duration:      {} minutes".format(int(round((slurm['terasort']['time']['end'] - slurm['terasort']['time']['start']) / 60.0))))
print("HDFS Read:     {} GiB".format(slurm['terasort']['io']['hdfs']['read']))
print("HDFS Write:    {} GiB".format(slurm['terasort']['io']['hdfs']['write']))
print("FILE Read:     {} GiB".format(slurm['terasort']['io']['file']['read']))
print("FILE Write:    {} GiB".format(slurm['terasort']['io']['file']['write']))
print("Shuffle Read:  {} GiB".format(slurm['terasort']['io']['shuffle']['read']))
print("Shuffle Write: {} GiB".format(slurm['terasort']['io']['shuffle']['write']))
print("Spill:         {} GiB".format(slurm['terasort']['io']['spill']))

print("")

print("Total")
print("=====")
print("Duration:      {} minutes".format(int(round((slurm['teragen']['time']['end'] - slurm['teragen']['time']['start'] + slurm['terasort']['time']['end'] - slurm['terasort']['time']['start']) / 60.0))))
print("HDFS Read:     {} GiB".format(slurm['teragen']['io']['hdfs']['read'] + slurm['terasort']['io']['hdfs']['read']))
print("HDFS Write:    {} GiB".format(slurm['teragen']['io']['hdfs']['write'] + slurm['terasort']['io']['hdfs']['write']))
print("FILE Read:     {} GiB".format(slurm['teragen']['io']['file']['read'] + slurm['terasort']['io']['file']['read']))
print("FILE Write:    {} GiB".format(slurm['teragen']['io']['file']['write'] + slurm['terasort']['io']['file']['write']))
print("Shuffle Read:  {} GiB".format(slurm['teragen']['io']['shuffle']['read'] + slurm['terasort']['io']['shuffle']['read']))
print("Shuffle Write: {} GiB".format(slurm['teragen']['io']['shuffle']['write'] + slurm['terasort']['io']['shuffle']['write']))
print("Spill:         {} GiB".format(slurm['teragen']['io']['spill'] + slurm['terasort']['io']['spill']))

print("")

if raw_data.empty:
    print("No CSV data imported, exiting.")
    quit()

# aggregate over all pids of desired hosts the metrics per time, key, source and category
hostnames = [
    'cumu01-00',
    'cumu01-01',
    'cumu01-02',
    'cumu01-03',
    'cumu01-04',
    'cumu01-05',
    'cumu01-06',
    'cumu01-07',
    'cumu01-08',
    'cumu01-09',
    'cumu01-10',
    'cumu01-11',
    'cumu01-12',
    'cumu01-13',
    'cumu01-14',
    'cumu01-15',
    'cumu02-00',
    'cumu02-01',
    'cumu02-02',
    'cumu02-03',
    'cumu02-04',
    'cumu02-05',
    'cumu02-06',
    'cumu02-07',
    'cumu02-08',
    'cumu02-09',
    'cumu02-10',
    'cumu02-11',
    'cumu02-12',
    'cumu02-13',
    'cumu02-14',
    'cumu02-15'
]
grouped_data = raw_data[raw_data['hostname'].isin(hostnames)].drop(['hostname', 'pid', 'fileDescriptor'], axis=1).groupby(['timeBin', 'key', 'source', 'category'], as_index=False).sum().set_index(['timeBin'], drop=False)

# regroup to obtain a DataFrame per key/source/category tuple
current_data = grouped_data.groupby(['key', 'source', 'category'], as_index=False)

# orders of plots when plotting SFS
sfs_orders = {
    ('map', 'sfs', 'write')    : 0,
    ('map', 'sfs', 'read')     : 1,
    ('reduce', 'sfs', 'write') : 2,
    ('reduce', 'sfs', 'read')  : 3,
    ('flink', 'sfs', 'write')  : 4,
    ('flink', 'sfs', 'read')   : 5
}

# orders of plots when plotting JVM
jvm_orders = {
    ('yarn', 'jvm', 'write')   : 0,
    ('yarn', 'jvm', 'read')    : 1,
    ('hdfs', 'jvm', 'write')   : 2,
    ('hdfs', 'jvm', 'read')    : 3,
    ('map', 'jvm', 'write')    : 4,
    ('map', 'jvm', 'read')     : 5,
    ('reduce', 'jvm', 'write') : 6,
    ('reduce', 'jvm', 'read')  : 7,
    ('flink', 'jvm', 'write')  : 8,
    ('flink', 'jvm', 'read')   : 9
}

for orders in [jvm_orders, sfs_orders]:
    # loop over all unique groups to build global index
    plot_index = None
    for group, group_data in current_data:
        plot_index = group_data.index if plot_index is None else plot_index.union(group_data.index)

    # dataframe used for plotting
    plot_data = pd.DataFrame(index=plot_index)
        
    # loop over all unique groups in previously defined order
    for group in sorted([g for g in current_data.groups if g in orders], key=lambda g: orders[g]):
        # group[0] contains the key, e.g. hadoop, spark, yarn, ...
        # group[1] contains jvm or sfs
        # group[2] contains read, write or other
        
        # get this group's data
        group_data = current_data.get_group(group)

        plot_data = plot_data.assign(**{ "{}: {}".format(group[0].capitalize(), group[2].capitalize()) : group_data['data'].cumsum() / 1073741824.0 })

    # compute the minute value for each time bin so we can plot against it
    plot_data = plot_data.assign(Minutes=lambda x: (x.index - plot_index.values[0]) / 60000)
    plot_data = plot_data.fillna(method='pad')

    plt.style.use('ggplot')
    plt.clf()

    ax = plot_data.plot.area(
        figsize=(32, 16),
        title="TeraSort I/O",
        legend=False,
        grid=False,
        x='Minutes',      # use computed minutes as x ticks
        linewidth=0       # to not show the line where the cumulative sum is still 0
    )
    patches, labels = ax.get_legend_handles_labels()
    ax.legend(reversed(patches), reversed(labels), loc='best') # reverse because we have added the plots bottom-up
    ax.set_ylabel("Stacked Cumulative Data (GiB)")
    ax2 = ax.twinx()
    ax2.set_ylim(ax.get_ylim())
    ax2.set_ylabel("Stacked Cumulative Data (GiB)")

    # add annotations, bottom to top
    yfrac = 1.0 / (plot_data.shape[1] - 1)
    i = 0
    total_data = 0
    for column in plot_data:
        if column == 'Minutes':
            continue

        total_data = total_data + plot_data[column].iloc[-1]
        ax.annotate(
            "{}: {} GiB".format(column, (int) (round(plot_data[column].iloc[-1]))),
            xy=(plot_data['Minutes'].iloc[-1], total_data), xycoords='data',
            xytext=(.975, .5 / (plot_data.shape[1] - 1) + i * yfrac), textcoords='axes fraction',
            size=20, va="center", ha="right",
            bbox=dict(boxstyle="round", fc="w", ec="0.5"),
            arrowprops=dict(arrowstyle="fancy", fc="w", ec="0.5")
        )
        i += 1
        
    # indicate teragen and terasort
    teragen_terasort_border = slurm['teragen']['time']['end'] + (slurm['terasort']['time']['start'] - slurm['teragen']['time']['end']) / 2.0
    ax.annotate("",
        xy=(teragen_terasort_border / 60.0, 0), xycoords='data',
        xytext=(teragen_terasort_border / 60.0, total_data), textcoords='data',
        arrowprops=dict(arrowstyle="-", linewidth=4),
    )
    ax.text((teragen_terasort_border - 180.0) / 60.0, .75 * total_data, "TeraGen", ha="right", va="center", size=20,
            bbox=dict(boxstyle="rarrow", fc="w", ec="0.5", alpha=0.9))
    ax.text((teragen_terasort_border + 180.0) / 60.0, .75 * total_data, "TeraSort", ha="left", va="center", size=20,
            bbox=dict(boxstyle="rarrow", fc="w", ec="0.5", alpha=0.9))
        
    plt.savefig("terasort-io-{}.pdf".format(dt.datetime.now()))
